# -*- coding: utf-8 -*-
"""transfer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Cm4ohacAxoPmSE1yK3WhgKGu-R2vuHUE

# Transfer learning

In this programming excersie, we train a model using transfer learning and convolutional neural networks to perform image classification on cat and dog dataset.

First, we import required libraries
"""

from tensorflow.keras.applications import InceptionResNetV2
from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img
from tensorflow.keras import layers
from tensorflow.keras import models
import zipfile
import matplotlib.pyplot as plt
plt.rcParams.update({'font.size': 16})
from tensorflow.math import confusion_matrix
import numpy as np
from sklearn.metrics import accuracy_score
import pandas as pd
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
from tensorflow.keras.utils import plot_model

"""We can download the dataset from the link below (I uploaded the data into my google drive)"""

#For colab
!gdown --id 1tXo2uBJg1RdeA9Ep3wwIqdCDseZ3rg1x

"""Now, we unzip the dataset """

# For colab
with zipfile.ZipFile('/content/cats_dogs_dataset.zip', 'r') as zip_ref:
    zip_ref.extractall()

"""Next, we download the InceptionResNetV2 with ImageNet weights. Include top is set to false to remove the last layer. """

pre_model = InceptionResNetV2(weights= "imagenet", include_top =False, input_shape=(150, 150, 3))

"""Here the pretrained model summary is shown, we can see it has 54 million parameters. """

pre_model.summary()

"""## Filter visualization

In order to visualize the filters of the first conv layer, we find the layer by name ('conv2d'). The weights of the layer are the learned filters. As we can see the shape is (3,3,3,32) which means there are 32 filter of 3x3, each of which has 3 channels. As there are only 3 channels, we can treat it as RGB for visualization purposes. Therefore we would get 32 color 3x3 images. For other layers, for example, 'conv2d_1' which is the second conv layer, the shape is (3,3,32,32) which means each of the 32, 3x3 filters have also 32 channels. In this case, each of the 32 channels can be visualized in gray scale which results in 32 * 32 visualizations (32 for number of filters, and 32 for each channel). Hopefully, in the first layer, we only have 3 channels and we can visualize it in RGB color space. It should be noted that the weights have arbitrary ranges, and normalizing them to be in range of 0 and 1 is necessary.
"""

for layer in pre_model.layers:
    if layer.name=='conv2d':
      filters = layer.get_weights()[0]

print('filter shape:', filters.shape)

fig=plt.figure(figsize=(12, 12))
columns = 8
rows = 4
for i in range(1, columns*rows+1):
    img = filters[:,:,:,i-1]
    img = (img - img.min()) / (img.max() - img.min())
    fig.add_subplot(rows, columns, i)
    plt.axis('off')
    plt.imshow(img)

plt.show()

"""We can see that some simple features are obtained. For example, in some, we can see a bright center while corner pixels are dark. In some other, we can see a bright/dark rows and columns which are like horizontal and vertical feature detectors.

## Image preprocessing

We use data generators to read the images from the source folders and feed them to our model. There is a generator for the training images and one for the testing images. We set the batch size parameter to 32 with the size of 150 and class label of binary.

We normalize the pixel values to be in the range of [0, 1]. In addition, we use some data augmentation techniques like horizontal flipping, zoom, and rotation on the training batches.
"""

img_size= 150
batch_size = 32


# For colab
train_dir = '/content/dataset/training_set'
test_dir = '/content/dataset/test_set'

train_datagen = ImageDataGenerator(
        rescale =1./255,
        horizontal_flip = True,
        rotation_range = 30,
        zoom_range=0.15,
        fill_mode='nearest'
)

test_datagen = ImageDataGenerator(rescale=1./255)


train_generator = train_datagen.flow_from_directory(
        directory=train_dir,
        target_size=(img_size, img_size),
        batch_size=batch_size,
        class_mode='binary') 

test_generator = test_datagen.flow_from_directory(
        directory=test_dir,
        target_size=(img_size, img_size),
        batch_size=batch_size,
        class_mode='binary', shuffle=False)

"""Here we visualize some images in the training batches. We can see the effect of the augmentation on the images. """

for j in range(4):
    aug_images = [train_generator[0][0][j] for i in range(4)]
    fig, axes = plt.subplots(1, 4, figsize=(24,24))
    axes = axes.flatten()
    for img, ax in zip(aug_images, axes):
        ax.imshow(img)
        ax.axis('off')
plt.tight_layout()
plt.show()

"""## Defining the transfer head and the new model"""

model = models.Sequential()
model.add(pre_model)
model.add(layers.Flatten())
model.add(layers.Dense(256, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))

pre_model.trainable=False
model.summary()

"""As we can see the pretrained layer is frozen and the trainable parameters are only 3.5 million.

## Model evaluation without training

Before training, we test the accuracy of the model on the dataset
"""

preds = np.round(model.predict(test_generator))
actual = test_generator.labels

cfmx = confusion_matrix(actual, preds, num_classes=2)
acc = accuracy_score(actual, preds)
print ('Accuracy:', acc )
print('Confusion matrix:', cfmx)

"""We can see the accuracy is like a random guess (around 50%). This is because the layers attached to the pretrained model are not trained yet, and the parameters required to find the correct classification are not learned yet.

## Model training

Now we compile and train the model. We use a regular adam optimizer with the default learning rate of 0.001 for the beginning and use `ReduceLROnPlateau` callback to reduce learning rate every 2 epochs if the validation accuracy is not improved. The loss is binary crossentropy as there are only two classes, the metric is accuracy. Max epochs is set to 25 with early stopping to stop the training if validation accuracy is not improved in 5 epochs.
"""

model.compile(optimizer='adam', loss='binary_crossentropy', metrics='acc')

rlr = ReduceLROnPlateau(monitor = 'val_acc', factor = 0.1, patience = 2, verbose = 2, 
                                min_delta = 1e-4, min_lr = 1e-6, mode = 'max')
es = EarlyStopping(monitor = 'val_acc', min_delta = 1e-4, patience = 5, mode = 'max', 
                    restore_best_weights = True, verbose = 2)

history = model.fit(
      train_generator,
      epochs=25,
      validation_data=test_generator,
      callbacks=[es,rlr],
      verbose=1)

"""Below, we can see the testing accuracy and confusion matrix. In addition, the training and testing loss and accuracy are shown in the plot. """

preds = np.round(model.predict(test_generator))
cfmx = confusion_matrix(actual, preds, num_classes=2)
acc = accuracy_score(actual, preds)
print ('Test Accuracy:', acc )
print('Confusion matrix:', cfmx)

"""We see that the accuracy on the test set is 0.9775 (**weights are restored to the best epoch**) with low amount of false positives and negatives. """

hist = pd.DataFrame(history.history)
fig, (ax1, ax2) = plt.subplots(figsize=(12,12),nrows=2, ncols=1)
hist['loss'].plot(ax=ax1,c='k',label='training loss')
hist['val_loss'].plot(ax=ax1,c='r',linestyle='--', label='validation loss')
ax1.legend()
hist['acc'].plot(ax=ax2,c='k',label='training accuracy')
hist['val_acc'].plot(ax=ax2,c='r',linestyle='--',label='validation accuracy')
ax2.legend()
plt.show()

"""## Sub-network training

To understand the pre_model's architecture bettter, we show the whole 
architecture below
"""

plot_model(pre_model, show_shapes=False, to_file='pre_model.png')

"""We see it is a huge network. We would like to cut the network at layer 40 (it's the end of first chunk of layers)"""

sub_pre_model = models.Model(pre_model.input, pre_model.layers[40].output)

sub_pre_model.summary()

"""Let's see how it looks like"""

plot_model(sub_pre_model, show_shapes=True, to_file='sub_pre_model.png')

"""Now we add the transfer head and freeze the layers of the pretrained sub network"""

sub_model = models.Sequential()
sub_model.add(sub_pre_model)
sub_model.add(layers.Flatten())
sub_model.add(layers.Dense(256, activation='relu'))
sub_model.add(layers.Dense(1, activation='sigmoid'))

sub_pre_model.trainable=False
sub_model.summary()

"""We train the model using the exact same setup"""

sub_model.compile(optimizer='adam', loss='binary_crossentropy', metrics='acc')

history2 = sub_model.fit(
      train_generator,
      epochs=25,
      validation_data=test_generator,
      callbacks=[es,rlr],
      verbose=1)

preds2 = np.round(sub_model.predict(test_generator))
cfmx2 = confusion_matrix(actual, preds2, num_classes=2)
acc2 = accuracy_score(actual, preds2)
print ('Test Accuracy:', acc2)
print('Confusion matrix:', cfmx2)

"""The testing accuracy is 0.8645 which is lower than the previous model, we can see from the loss per epoch that the network is unable to reach a better accuracy. """

hist2 = pd.DataFrame(history2.history)
fig, (ax1, ax2) = plt.subplots(figsize=(12,12),nrows=2, ncols=1)
hist2['loss'].plot(ax=ax1,c='k',label='training loss')
hist2['val_loss'].plot(ax=ax1,c='r',linestyle='--', label='validation loss')
ax1.legend()
hist2['acc'].plot(ax=ax2,c='k',label='training accuracy')
hist2['val_acc'].plot(ax=ax2,c='r',linestyle='--',label='validation accuracy')
ax2.legend()
plt.show()

"""## Comparison

We can see the first network did a better job in terms of reducing the optimization loss, maximizing the validation accuracy, and having fewer off-diagonal confusion matrix elements.

"""