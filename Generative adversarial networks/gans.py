# -*- coding: utf-8 -*-
"""GANs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/Sinamhd9/Computer-vision-projects/blob/main/Generative%20adversarial%20networks/GANs.ipynb

# Generative adverserial network (GAN)
"""

from tensorflow.keras.layers import Dense, Conv2D, BatchNormalization, LeakyReLU, Dropout, InputLayer, Flatten, Conv2DTranspose, Reshape
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.datasets import fashion_mnist
import matplotlib.pyplot as plt
import numpy as np
import tensorflow.keras.backend as K
from tensorflow.keras.models import load_model
from tensorflow.keras.optimizers import Adam
import pandas as pd

"""## Fashion MNIST data"""

(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

"""### Visualization"""

n = 100
fig, axs = plt.subplots(10, 10, figsize=(10, 12))
axs = axs.ravel()

for i in range(n):
  axs[i].imshow(x_train[i], cmap='gray_r')
  axs[i].set_title(y_train[i])
  axs[i].axis('off')

"""## Dense GAN model

### Generator

Based on the instructions and some trial and error, I ended up with this architecture consisting of several dense , Leaky relu , batch norm, and dropout blocks.
"""

def generator_model(input_dim):
  model = Sequential ([
                       Dense(100, activation=LeakyReLU(), input_shape=(input_dim,)),
                       BatchNormalization(),
                       Dense(256, activation=LeakyReLU()),
                       BatchNormalization(),
                       Dropout(0.2),
                       Dense(512, activation=LeakyReLU()),
                       BatchNormalization(),
                       Dropout(0.2),
                       Dense(1024, activation=LeakyReLU()),
                       BatchNormalization(),
                       Dropout(0.2),
                       Dense(784, 'sigmoid'),
                       Reshape((28, 28))
                       ])
  return model

"""### Discriminator

Same here, several Dense, dropout layers without Batchnorms.
"""

def discriminator_model(input_dim):
  model = Sequential ([
                            InputLayer(input_dim),
                            Flatten(),
                            Dense (1024, activation=LeakyReLU()),
                            Dropout(0.2),
                            Dense (512, activation=LeakyReLU()),
                            Dropout(0.2),
                            Dense (256, activation=LeakyReLU()),
                            Dropout(0.2),
                            Dense(1, 'sigmoid'),
                       ])
  model.compile(optimizer = Adam(learning_rate= 0.0001, beta_1 = 0.9), loss='binary_crossentropy')
  return model

"""### GAN Model

Simply connecting generator and discriminator and freezing the weights of the discriminator.
"""

def gan_model(gen, disc):
  disc.trainable = False
  model = Sequential([
                      gen,
                      disc
                      ])
  model.compile(optimizer = Adam(learning_rate= 0.0001, beta_1 = 0.9), loss= 'binary_crossentropy')
  return model

"""### Preprocessing

Chanding the range of the data to 0 and 1, turning them to float32 and adding an extra axis to feed them to the network
"""

def preprocess(data):
  return np.expand_dims((data/255).astype('float32') , axis=-1)

"""Getting real images randomly from the dataset"""

def get_realData(data, n):
  ix = np.random.randint(0, data.shape[0], size=n)
  return data[ix]

"""Generate noise based on normal distribution"""

def generate_noise(n, dim = 100):
  noise = np.zeros((n, dim))
  for i in range(n):
    noise[i]= np.random.randn(dim)
  return noise

"""Output of the generator when fed with noise to generate fake examples"""

def generate_fake(generator, noise):
  fake_images = generator.predict(noise)
  return fake_images

"""### Training

Training function, saving all losses of batches, and plotting the generated images every 10 epochs
"""

def train(gan, gen, disc, data, dim = 100, max_epochs=100, batch_size=128):
  n_batches = int(data.shape[0] / batch_size)
  n_batches_half = int(n_batches / 2)
  dis_loss_real_all = []
  dis_loss_fake_all = []
  gan_loss_all = []
  for i in range(max_epochs):
    for j in range(n_batches):
      x_real = get_realData(data, n_batches_half)
      y_real = np.ones((n_batches_half, 1))
      disc_loss_real = disc.train_on_batch(x_real, y_real)
      noise_input = generate_noise(n_batches_half, dim)
      x_fake = generate_fake(gen, noise_input)
      y_fake = np.zeros((n_batches_half, 1))
      disc_loss_fake = disc.train_on_batch(x_fake, y_fake)
      x_gan = generate_noise(n_batches, dim)
      y_gan = np.ones((n_batches, 1))
      gan_loss = gan.train_on_batch(x_gan, y_gan)
      dis_loss_real_all.append(disc_loss_real)
      dis_loss_fake_all.append(disc_loss_fake)
      gan_loss_all.append(gan_loss)
    print(f'epoch: {i}, discriminator loss real: {disc_loss_real}, discriminator loss fake: {disc_loss_fake}, gan loss: {gan_loss}')
    if (i%10 == 0):
      pred = gen.predict(x_gan)
      sample_images = pred.reshape(n_batches, 28, 28)[:100]
      n = 100
      fig, axs = plt.subplots(10, 10, figsize=(10, 12))
      axs = axs.ravel()
      for z in range(n):
        axs[z].imshow(sample_images[z], cmap='gray_r')
        axs[z].axis('off')
      plt.show()
  K.clear_session()
  return {'gan_loss': gan_loss_all, 'dis_loss_real': dis_loss_real_all, 'dis_loss_fake': dis_loss_fake_all}

data = preprocess(x_train)
print(data.shape)

input_dim_disc = (28, 28, 1)
discriminator = discriminator_model(input_dim_disc)
discriminator.summary()

input_dim_gen = 100
generator = generator_model(input_dim_gen)
generator.summary()

gan = gan_model(generator, discriminator)
gan.summary()

"""### Results of Dense GAN model

Loss of discriminator on every epoch is shown on both fake and real examples, as well as the loss of the whole GAN model. We can see it is somehow reaching an equilibrium and making better fake images.
"""

history = train(gan, generator, discriminator, data, dim = 100, max_epochs = 100, batch_size = 128)

"""#### Final epoch"""

pred = generator.predict(generate_noise(100, 100))
sample_images = pred.reshape(100, 28, 28)
n = 100
fig, axs = plt.subplots(10, 10, figsize=(10, 12))
axs = axs.ravel()
for z in range(n):
    axs[z].imshow(sample_images[z], cmap='gray_r')
    axs[z].axis('off')
plt.show()

"""#### Loss plot"""

hist = pd.DataFrame(history)

fig, axs = plt.subplots(figsize=(8,8))
hist['gan_loss'].plot(c='k',label='GAN loss')
hist['dis_loss_real'].plot(c='r',linestyle='--', label='Discriminator loss on real images')
hist['dis_loss_fake'].plot(c='b',linestyle='-.', label='Discriminator loss on fake images')
axs.legend()
plt.show()

"""## CNN-based GAN

Now we repeat everything to build CNN GAN models. The same concepts are applied, conv2d and conv2dTranspose layers are used instead of the dense layers. 
"""

def CNN_generator_model(input_dim):
  model = Sequential ([
                       Dense(7*7*128, activation=LeakyReLU(), input_shape=(input_dim,)),
                       Reshape((7, 7, 128)),
                       Conv2DTranspose(128, 5, activation=LeakyReLU(), strides=2, padding='same'),
                       Conv2DTranspose(128, 5, activation=LeakyReLU(), strides=2, padding='same'),
                       Conv2D(1, (7,7), activation='sigmoid', padding='same')
                       ])
  return model

def CNN_discriminator_model(input_dim):
  model = Sequential ([
                        Conv2D(128, 5, activation=LeakyReLU(), strides=2, padding='same', input_shape=input_dim),
                        Conv2D(128, 5, activation=LeakyReLU(), strides=2 , padding='same'),
                        Flatten(),
                        Dropout(0.2),
                        Dense(1, 'sigmoid'),
                       ])
  model.compile(optimizer = Adam(learning_rate= 0.0002, beta_1 = 0.5), loss='binary_crossentropy')
  return model

def cnn_gan_model(gen, disc):
  disc.trainable = False
  model = Sequential([
                      gen,
                      disc
                      ])
  model.compile(optimizer = Adam(learning_rate= 0.0002, beta_1 = 0.5), loss= 'binary_crossentropy')
  return model

input_dim_disc = (28, 28, 1)
cnn_discriminator = CNN_discriminator_model(input_dim_disc)
cnn_discriminator.summary()

input_dim_gen = 100
cnn_generator = CNN_generator_model(input_dim_gen)
cnn_generator.summary()

cnn_gan = cnn_gan_model(cnn_generator, cnn_discriminator)
cnn_gan.summary()

"""### Results of CNN GAN model"""

cnn_history = train(cnn_gan, cnn_generator, cnn_discriminator, data, dim = 100, max_epochs = 40, batch_size = 128)

"""#### Final epoch"""

pred = cnn_generator.predict(generate_noise(100, 100))
sample_images = pred.reshape(100, 28, 28)
n = 100
fig, axs = plt.subplots(10, 10, figsize=(10, 12))
axs = axs.ravel()
for z in range(n):
    axs[z].imshow(sample_images[z], cmap='gray_r')
    axs[z].axis('off')
plt.show()

"""#### Loss plot"""

cnn_hist = pd.DataFrame(cnn_history)

fig, axs = plt.subplots(figsize=(8,8))
cnn_hist['gan_loss'].plot(c='k',label='GAN loss')
cnn_hist['dis_loss_real'].plot(c='r',linestyle='--', label='Discriminator loss on real images')
cnn_hist['dis_loss_fake'].plot(c='b',linestyle='-.', label='Discriminator loss on fake images')
axs.legend()
plt.show()

"""## Conclusion

We can see that how output images are getting more realistic every 10 epochs, CNN model is more creative and it converged sooner so I stopped the training at 40 epochs.
"""